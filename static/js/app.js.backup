// Variables globales
let audioContext;
let audioInput;
let recorder;
let audioChunks = [];
let isRecording = false;
let conversationHistory = []; // Historique de la conversation
let autoRestart = true; // Red√©marrer automatiquement apr√®s chaque r√©ponse
let silenceDetectionEnabled = true; // Activer la d√©tection de silence
let silenceTimer = null;
let analyser = null;
let silenceThreshold = 0.01; // Seuil de silence (ajustable)
let silenceDuration = 1500; // Dur√©e de silence avant arr√™t (ms)

// √âl√©ments DOM
const startBtn = document.getElementById('start-btn');
const stopBtn = document.getElementById('stop-btn');
const statusBox = document.getElementById('status');
const statusText = document.getElementById('status-text');
const transcriptionCard = document.getElementById('transcription-card');
const transcriptionText = document.getElementById('transcription-text');
const translationCard = document.getElementById('translation-card');
const translationText = document.getElementById('translation-text');
const audioCard = document.getElementById('audio-card');
const audioPlayer = document.getElementById('audio-player');
const logContent = document.getElementById('log-content');

// Gestion des logs
function addLog(message, type = 'info') {
    const timestamp = new Date().toLocaleTimeString('fr-FR');
    const logEntry = document.createElement('div');
    logEntry.className = `log-entry ${type}`;
    logEntry.innerHTML = `<span class="log-timestamp">[${timestamp}]</span>${message}`;
    logContent.insertBefore(logEntry, logContent.firstChild);
}

// Mise √† jour du statut
function updateStatus(icon, text, className = '') {
    statusBox.className = 'status-box ' + className;
    statusBox.querySelector('.status-icon').textContent = icon;
    statusText.textContent = text;
}

// Classe pour enregistrer en WAV
class AudioRecorder {
    constructor(source, cfg) {
        this.config = cfg || {};
        this.bufferLen = this.config.bufferLen || 4096;
        this.numChannels = this.config.numChannels || 1;
        this.sampleRate = source.context.sampleRate;

        this.context = source.context;
        this.node = this.context.createScriptProcessor(this.bufferLen, this.numChannels, this.numChannels);

        this.buffers = [];
        for (let i = 0; i < this.numChannels; i++) {
            this.buffers[i] = [];
        }

        const self = this;
        this.node.onaudioprocess = function(e) {
            if (!self.recording) return;

            for (let i = 0; i < self.numChannels; i++) {
                self.buffers[i].push(new Float32Array(e.inputBuffer.getChannelData(i)));
            }
        };

        source.connect(this.node);
        this.node.connect(this.context.destination);
    }

    record() {
        this.recording = true;
    }

    stop() {
        this.recording = false;
    }

    clear() {
        for (let i = 0; i < this.numChannels; i++) {
            this.buffers[i] = [];
        }
    }

    getBuffer() {
        const buffers = [];
        for (let i = 0; i < this.numChannels; i++) {
            buffers.push(this.mergeBuffers(this.buffers[i]));
        }
        return buffers;
    }

    mergeBuffers(bufferArray) {
        const length = bufferArray.reduce((acc, buf) => acc + buf.length, 0);
        const result = new Float32Array(length);
        let offset = 0;
        for (let i = 0; i < bufferArray.length; i++) {
            result.set(bufferArray[i], offset);
            offset += bufferArray[i].length;
        }
        return result;
    }

    exportWAV() {
        const buffers = this.getBuffer();
        const interleaved = this.interleave(buffers);
        const dataview = this.encodeWAV(interleaved);
        const audioBlob = new Blob([dataview], { type: 'audio/wav' });
        return audioBlob;
    }

    interleave(buffers) {
        if (this.numChannels === 1) {
            return buffers[0];
        }
        const length = buffers[0].length;
        const result = new Float32Array(length * this.numChannels);
        for (let i = 0; i < length; i++) {
            for (let j = 0; j < this.numChannels; j++) {
                result[i * this.numChannels + j] = buffers[j][i];
            }
        }
        return result;
    }

    encodeWAV(samples) {
        const buffer = new ArrayBuffer(44 + samples.length * 2);
        const view = new DataView(buffer);

        // RIFF identifier
        this.writeString(view, 0, 'RIFF');
        // file length
        view.setUint32(4, 36 + samples.length * 2, true);
        // RIFF type
        this.writeString(view, 8, 'WAVE');
        // format chunk identifier
        this.writeString(view, 12, 'fmt ');
        // format chunk length
        view.setUint32(16, 16, true);
        // sample format (raw)
        view.setUint16(20, 1, true);
        // channel count
        view.setUint16(22, this.numChannels, true);
        // sample rate
        view.setUint32(24, this.sampleRate, true);
        // byte rate (sample rate * block align)
        view.setUint32(28, this.sampleRate * this.numChannels * 2, true);
        // block align (channel count * bytes per sample)
        view.setUint16(32, this.numChannels * 2, true);
        // bits per sample
        view.setUint16(34, 16, true);
        // data chunk identifier
        this.writeString(view, 36, 'data');
        // data chunk length
        view.setUint32(40, samples.length * 2, true);

        // write the PCM samples
        this.floatTo16BitPCM(view, 44, samples);

        return view;
    }

    writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
            view.setUint8(offset + i, string.charCodeAt(i));
        }
    }

    floatTo16BitPCM(output, offset, input) {
        for (let i = 0; i < input.length; i++, offset += 2) {
            const s = Math.max(-1, Math.min(1, input[i]));
            output.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
        }
    }
}

// D√©tection de silence
function checkAudioLevel() {
    if (!analyser || !isRecording) return;

    const bufferLength = analyser.fftSize;
    const dataArray = new Uint8Array(bufferLength);
    analyser.getByteTimeDomainData(dataArray);

    // Calculer le volume moyen
    let sum = 0;
    for (let i = 0; i < bufferLength; i++) {
        const normalized = (dataArray[i] - 128) / 128;
        sum += normalized * normalized;
    }
    const rms = Math.sqrt(sum / bufferLength);

    // D√©tection de silence
    if (rms < silenceThreshold) {
        // Silence d√©tect√©
        if (!silenceTimer && silenceDetectionEnabled) {
            silenceTimer = setTimeout(() => {
                if (isRecording) {
                    addLog('üîá Silence d√©tect√© - Arr√™t automatique', 'info');
                    stopRecording();
                }
            }, silenceDuration);
        }
    } else {
        // Son d√©tect√© - annuler le timer de silence
        if (silenceTimer) {
            clearTimeout(silenceTimer);
            silenceTimer = null;
        }
    }

    // Continuer √† v√©rifier
    if (isRecording) {
        requestAnimationFrame(checkAudioLevel);
    }
}

// D√©marrer l'enregistrement
async function startRecording() {
    try {
        addLog('Demande d\'acc√®s au microphone...', 'info');

        // Demander l'acc√®s au microphone
        const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
                echoCancellation: true,
                noiseSuppression: true,
                sampleRate: 16000
            }
        });

        addLog('‚úì Microphone activ√©', 'success');

        // Cr√©er l'AudioContext
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        audioInput = audioContext.createMediaStreamSource(stream);

        // Cr√©er l'analyseur pour la d√©tection de silence
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        analyser.smoothingTimeConstant = 0.8;
        audioInput.connect(analyser);

        // Cr√©er le recorder
        recorder = new AudioRecorder(audioInput, {
            numChannels: 1,
            bufferLen: 4096
        });

        // D√©marrer l'enregistrement
        recorder.clear();
        recorder.record();
        isRecording = true;

        // D√©marrer la d√©tection de silence
        if (silenceDetectionEnabled) {
            silenceTimer = null;
            checkAudioLevel();
        }

        // Mettre √† jour l'interface
        startBtn.disabled = true;
        stopBtn.disabled = false;
        updateStatus('üî¥', 'Parlez... (arr√™t auto apr√®s silence)', 'recording');
        addLog('üé§ Enregistrement d√©marr√© - D√©tection de silence active', 'success');

    } catch (error) {
        console.error('Erreur microphone:', error);
        addLog('‚ùå Erreur: ' + error.message, 'error');
        updateStatus('‚ùå', 'Erreur d\'acc√®s au microphone', '');
    }
}

// Arr√™ter l'enregistrement
function stopRecording() {
    if (recorder && isRecording) {
        recorder.stop();
        isRecording = false;

        // Arr√™ter la d√©tection de silence
        if (silenceTimer) {
            clearTimeout(silenceTimer);
            silenceTimer = null;
        }

        // Mettre √† jour l'interface
        startBtn.disabled = false;
        stopBtn.disabled = true;
        updateStatus('‚è≥', 'Traitement en cours...', 'processing');

        // Traiter l'audio
        processAudio();
    }
}

// Traiter l'audio enregistr√©
async function processAudio() {
    try {
        // Obtenir le WAV
        const wavBlob = recorder.exportWAV();

        addLog(`üì¶ Audio captur√©: ${(wavBlob.size / 1024).toFixed(2)} KB`, 'info');

        if (wavBlob.size < 100) {
            throw new Error('Audio trop court ou vide');
        }

        const formData = new FormData();
        formData.append('audio', wavBlob, 'recording.wav');

        // √âtape 1: Transcription
        updateStatus('üìù', 'Transcription en cours...', 'processing');
        addLog('Envoi √† Whisper pour transcription...', 'info');

        const transcribeResponse = await fetch('/api/transcribe', {
            method: 'POST',
            body: formData
        });

        const transcribeData = await transcribeResponse.json();

        if (!transcribeData.success) {
            throw new Error(transcribeData.error || 'Erreur de transcription');
        }

        const userMessage = transcribeData.text;
        addLog('‚úì Vous: ' + userMessage, 'success');

        // Afficher le message utilisateur
        transcriptionText.textContent = userMessage;
        transcriptionCard.style.display = 'block';

        // Ajouter le message utilisateur √† l'historique
        conversationHistory.push({
            role: 'user',
            content: userMessage
        });

        // √âtape 2: Obtenir la r√©ponse de l'agent en streaming
        updateStatus('ü§ñ', 'L\'agent r√©fl√©chit et parle...', 'processing');
        addLog('Streaming de la r√©ponse...', 'info');

        let agentResponse = '';
        let sentencesQueue = [];
        let isPlayingAudio = false;
        let audioQueue = [];

        // Fonction pour jouer les audios en s√©quence
        const playNextAudio = () => {
            if (audioQueue.length > 0 && !isPlayingAudio) {
                isPlayingAudio = true;
                const audioBlob = audioQueue.shift();
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlayer.src = audioUrl;
                audioCard.style.display = 'block';
                audioPlayer.play();

                audioPlayer.onended = () => {
                    isPlayingAudio = false;
                    playNextAudio();
                };
            }
        };

        // Fonction pour g√©n√©rer l'audio d'une phrase
        const generateSpeechForSentence = async (sentence) => {
            try {
                const speakResponse = await fetch('/api/speak_stream', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ text: sentence })
                });

                const audioBlob = await speakResponse.blob();
                audioQueue.push(audioBlob);
                playNextAudio();
            } catch (error) {
                console.error('Erreur g√©n√©ration audio:', error);
            }
        };

        // Stream la r√©ponse de GPT avec fetch
        const response = await fetch('/api/chat_stream', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                text: userMessage,
                history: conversationHistory
            })
        });

        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let currentSentence = '';

        while (true) {
            const { done, value } = await reader.read();

            if (done) {
                // Envoyer la derni√®re phrase si elle existe
                if (currentSentence.trim()) {
                    generateSpeechForSentence(currentSentence.trim());
                }
                break;
            }

            const chunk = decoder.decode(value, { stream: true });
            const lines = chunk.split('\n');

            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const data = line.substring(6);

                    if (data === '[DONE]') {
                        continue;
                    }

                    agentResponse += data;
                    currentSentence += data;

                    // Afficher progressivement
                    translationText.textContent = agentResponse;
                    translationCard.style.display = 'block';

                    // D√©tecter fin de phrase
                    if (/[.!?]\s*$/.test(currentSentence.trim())) {
                        const sentence = currentSentence.trim();
                        if (sentence.length > 5) {
                            generateSpeechForSentence(sentence);
                            currentSentence = '';
                        }
                    }
                }
            }
        }

        addLog('‚úì Agent: ' + agentResponse, 'success');

        // Ajouter la r√©ponse compl√®te √† l'historique
        conversationHistory.push({
            role: 'assistant',
            content: agentResponse
        });

        // Attendre que tous les audios soient jou√©s
        updateStatus('üîä', 'Lecture de la r√©ponse...', 'speaking');

        // Fonction pour attendre la fin de tous les audios
        const waitForAllAudios = () => {
            return new Promise((resolve) => {
                const checkInterval = setInterval(() => {
                    if (audioQueue.length === 0 && !isPlayingAudio) {
                        clearInterval(checkInterval);
                        resolve();
                    }
                }, 100);
            });
        };

        await waitForAllAudios();

        updateStatus('‚úÖ', '√Ä vous ! Cliquez pour continuer...', '');
        addLog('‚úì Tour termin√© - En attente de votre r√©ponse', 'success');

        // Red√©marrer automatiquement l'enregistrement apr√®s 1 seconde
        if (autoRestart) {
            setTimeout(() => {
                addLog('üîÑ Red√©marrage automatique...', 'info');
                startRecording();
            }, 1000);
        }

    } catch (error) {
        console.error('Erreur de traitement:', error);
        addLog('‚ùå Erreur: ' + error.message, 'error');
        updateStatus('‚ùå', 'Erreur: ' + error.message, '');
        startBtn.disabled = false;
    }
}

// Event listeners
startBtn.addEventListener('click', startRecording);
stopBtn.addEventListener('click', stopRecording);

// Toggle de la d√©tection de silence
const silenceToggle = document.getElementById('silence-detection-toggle');
if (silenceToggle) {
    silenceToggle.addEventListener('change', (e) => {
        silenceDetectionEnabled = e.target.checked;
        addLog(silenceDetectionEnabled ? '‚úì D√©tection de silence activ√©e' : '‚ö†Ô∏è D√©tection de silence d√©sactiv√©e', 'info');
    });
}

// Raccourci clavier: Espace pour d√©marrer/arr√™ter
document.addEventListener('keydown', (e) => {
    if (e.code === 'Space' && !e.repeat) {
        e.preventDefault();
        if (!isRecording && !startBtn.disabled) {
            startRecording();
        }
    }
});

document.addEventListener('keyup', (e) => {
    if (e.code === 'Space' && isRecording) {
        e.preventDefault();
        stopRecording();
    }
});

// Message de bienvenue
addLog('üéâ Application pr√™te ! Cliquez sur "Commencer √† parler" ou maintenez la barre d\'espace', 'success');
